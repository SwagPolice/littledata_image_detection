   "source": [
    "'''This script is adapted from the blog post\n",
    "\n",
    "\"Building powerful image classification models using very little data\"\n",
    "\n",
    "from blog.keras.io.\n",
    "\n",
    "So that we have 1000 training examples for each class, and 300 validation examples for each class.\n",
    "\n",
    "In summary, this is our directory structure:\n",
    "\n",
    "```\n",
    "\n",
    "data/\n",
    "\n",
    "    train/\n",
    "\n",
    "        object1/\n",
    "\n",
    "            object1_1.jpg\n",
    "\n",
    "            object1_2.jpg\n",
    "\n",
    "            ...\n",
    "\n",
    "        object2/\n",
    "\n",
    "            object2_1.jpg\n",
    "\n",
    "            object2_2.jpg\n",
    "\n",
    "            ...\n",
    "\n",
    "    validation/\n",
    "\n",
    "        object1/\n",
    "\n",
    "            object1_1.jpg\n",
    "\n",
    "            object1_2.jpg\n",
    "\n",
    "            ...\n",
    "\n",
    "        object2/\n",
    "\n",
    "            object2_1.jpg\n",
    "\n",
    "            object2_2.jpg\n",
    "\n",
    "            ...\n",
    "\n",
    "```\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "print('keras version:' + keras.__version__)\n",
    "print('tensorflow version:' + tensorflow.__version__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '/data/train'\n",
    "\n",
    "validation_data_dir = '/data/validation'\n",
    "\n",
    "nb_train_samples = 8000\n",
    "\n",
    "nb_validation_samples = 2400\n",
    "\n",
    "epochs = 80\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "\n",
    "    input_shape = (3, img_width, img_height)\n",
    "\n",
    "else:\n",
    "    \n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8))\n",
    "\n",
    "model.add(Activation('softmax')) #use sigmoid when binary and softmax when categorical\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\n",
    "              optimizer='rmsprop',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "\n",
    "    rescale=1. / 255,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True)\n",
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "\n",
    "# only rescaling\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "\n",
    "    train_data_dir,\n",
    "\n",
    "    target_size=(img_width, img_height),\n",
    "\n",
    "    batch_size=batch_size,\n",
    "\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "\n",
    "    validation_data_dir,\n",
    "\n",
    "    target_size=(img_width, img_height),\n",
    "\n",
    "    batch_size=batch_size,\n",
    "\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "\n",
    "    train_generator,\n",
    "\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "\n",
    "    epochs=epochs,\n",
    "\n",
    "    validation_data=validation_generator,\n",
    "\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "\n",
    "model.save('detect_model.h5')\n",
    "model.save_weights('detect_weights.h5')"
   ]
